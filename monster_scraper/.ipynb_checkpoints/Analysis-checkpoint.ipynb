{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import itertools # to find combinations of features\n",
    "%matplotlib inline\n",
    "import networkx as nx # to plot network graphs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Using the MonsterScraper_ForEveryState.ipynb file, I web scraped data up to 250 job posts from every state for the search 'data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### The analysis will examine a few simple questions:\n",
    "    - #### Which data science skills appear the most?\n",
    "    - #### Which data science skills appear together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## First, we will import the data from the file scraped_data_4-6-2019.txt into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data = pd.read_csv('scraped_data_4-6-2019.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## I am skeptical about the 'c' column, so I will drop it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Lets store the NY data for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NY = data[data['state'] == 'NY'].drop(columns=['state','job_title'])\n",
    "data_US = data.drop(columns=['state','job_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Lets look at some of the sample sizes from each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "DE    115\n",
       "AL     91\n",
       "KS     38\n",
       "WY     27\n",
       "OH     27\n",
       "MO     27\n",
       "MI     26\n",
       "ME     25\n",
       "NC     25\n",
       "AZ     25\n",
       "HI     25\n",
       "MD     25\n",
       "SC     25\n",
       "MN     25\n",
       "WA     25\n",
       "AK     25\n",
       "NE     25\n",
       "NH     25\n",
       "RI     25\n",
       "TX     25\n",
       "OR     25\n",
       "PA     25\n",
       "UT     22\n",
       "IA     17\n",
       "SD     17\n",
       "MA      4\n",
       "VA      3\n",
       "CO      3\n",
       "CA      3\n",
       "IN      2\n",
       "GA      2\n",
       "VT      2\n",
       "MS      2\n",
       "NV      1\n",
       "Name: python, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('state').count()['python'].sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We apparently mined some extra data from WA and NY, but thats fine with me!\n"
     ]
    }
   ],
   "source": [
    "print('We apparently mined some extra data from WA and NY, but thats fine with me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1: What skills appear the most?\n",
    "- ## Since I am a proud New Yorker, I will answer the question:'what skills are most important in New York?' \n",
    "- ## So, we need to find the frequency of occurance of each skill in the set of job posts in New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'NY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d60f90a1c1e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Some Pandas Magic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprob_skill_and_NY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mikegiacomazza/anaconda3/envs/RotatingIpEnv/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'NY'"
     ]
    }
   ],
   "source": [
    "# Some Pandas Magic\n",
    "prob_skill_and_NY = pd.DataFrame([data.groupby('state').mean().T.NY.sort_values(ascending=False)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a definition to normalize the data between zero and one\n",
    "# This will be useful in multiple places\n",
    "def normalizeZeroOne(array):\n",
    "    return ((array)-min(array))/(max(array)-min(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets plot the answer to our question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the subplots\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(30,15))\n",
    "\n",
    "# Make the colorbar\n",
    "\n",
    "## We will first create a norm, that will color the data within a interval that we define\n",
    "## All possible values of the data span between 0 and 1, so we will use this for the input \n",
    "## of the norm in out colormap\n",
    "norm = mpl.colors.Normalize(vmin = 0,\n",
    "                            vmax = 1)\n",
    "\n",
    "## Second, we select the colormap that we want\n",
    "cmap = mpl.cm.Set1\n",
    "\n",
    "## Now we get the RGBA values using our norm and colormap for the values \n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "bar_colors = m.to_rgba(prob_skill_and_NY['NY'].values)\n",
    "\n",
    "\n",
    "# Add the plots to the subplots\n",
    "prob_skill_and_NY['NY'].plot(ax = ax,\n",
    "                             kind='bar',                             \n",
    "                             color = bar_colors,\n",
    "                             fontsize=30,\n",
    "                             edgecolor = 'k',\n",
    "                             ylim = [0,1],\n",
    "                             width = .7)                \n",
    "\n",
    "# Set tick frequency to .1 between 0 and 1\n",
    "plt.yticks(np.arange(0, 1.1, .1))\n",
    "\n",
    "# Add Gridlines\n",
    "ax.grid(b=None, which='major', axis='both')\n",
    "\n",
    "# Add ylabel\n",
    "ax.set_ylabel('NY',fontsize=40)\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Skill Frequency In Job Posts',fontsize=50,y=1.01)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('figures/NY_hist.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I used color labels that adjust according to the skills value, so we can visually identify a few groups in the data:\n",
    "- ## We can see that Python, R and SQL are the first tier with the highest demand.\n",
    "- ## Hadoop, Spark, and Java are second tier.\n",
    "- ## Tableau, SAS, Scala, and Hive are third tier.\n",
    "- ## Matlab, Excel, NoSQL, c++, and tensorflow are fourth tier\n",
    "- ## Then the remaining skills should appear for less than 1 in 10 job posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at this same plot, but also for job posts from the entire united states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability of occurance of a skill for all of the states\n",
    "prob_skill_and_US = pd.DataFrame(data.mean().sort_values(ascending=False),columns=['US'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make the subplots\n",
    "fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(20,15))\n",
    "\n",
    "# Make the colorbar\n",
    "\n",
    "## We will first create a norm, that will color the data within a interval that we define\n",
    "## All possible values of the data span between 0 and 1, so we will use this for the input \n",
    "## of the norm in out colormap\n",
    "norm = mpl.colors.Normalize(vmin = 0,\n",
    "                            vmax = 1)\n",
    "\n",
    "## Second, we select the colormap that we want\n",
    "cmap = mpl.cm.Set1\n",
    "\n",
    "## Now we get the RGBA values using our norm and colormap for the values \n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "NY_bar_colors = m.to_rgba(prob_skill_and_NY['NY'].values)\n",
    "US_bar_colors = m.to_rgba(prob_skill_and_US['US'].values)\n",
    "\n",
    "# Add the plots to the subplots\n",
    "prob_skill_and_NY['NY'].plot(ax=ax[0],\n",
    "                             kind='bar',\n",
    "                             color = NY_bar_colors,\n",
    "                             fontsize=30,\n",
    "                             width = .7,\n",
    "                             edgecolor='k',\n",
    "                             ylim = [0,1])\n",
    "\n",
    "prob_skill_and_US['US'].plot(ax=ax[1],\n",
    "                             kind='bar',\n",
    "                             color = US_bar_colors,\n",
    "                             fontsize=30,\n",
    "                             width = .7,\n",
    "                             edgecolor='k',\n",
    "                            ylim = [0,1])\n",
    "\n",
    "\n",
    "\n",
    "# Add super title\n",
    "fig.suptitle('Skill Frequency In Job Posts', fontsize=50, y=1.05)\n",
    "# Add the ylabels to the plots\n",
    "ax[0].set_ylabel('NY',fontsize=40)\n",
    "ax[1].set_ylabel('US',fontsize=40)\n",
    "\n",
    "# Add Gridlines\n",
    "ax[0].grid(b=None, which='major', axis='both')\n",
    "ax[1].grid(b=None, which='major', axis='both')\n",
    "\n",
    "# Make the subplot layout tightly fit\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('figures/NY_vs_US_hist.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we say about these two plots?\n",
    "- ## Although the scale is different, the shape seems to be is relatively the same\n",
    "- ## Lets get rid of this difference in scale and just look at the shape of the plots. To do this we will normalize the data using the equation: \n",
    "## $$ x' = \\frac{(x-min(x))} {(max(x)-min(x))} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Normalize with respect to python's probability for each set\n",
    "prob_skill_and_NY_normalized = normalizeZeroOne(prob_skill_and_NY['NY'])\n",
    "prob_skill_and_US_normalized = normalizeZeroOne(prob_skill_and_US['US'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the subplots\n",
    "fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(20,15))\n",
    "\n",
    "# Add the plots to the subplots\n",
    "prob_skill_and_NY_normalized.plot(ax =ax[0],kind='bar',fontsize=30,\n",
    "                                  color = plt.cm.Set1(prob_skill_and_NY_normalized),\n",
    "                                  edgecolor = 'k',\n",
    "                                  width = .7,\n",
    "                                  ylim=[0,1])\n",
    "prob_skill_and_US_normalized.plot(ax=ax[1],kind='bar',fontsize=30,\n",
    "                                  color=plt.cm.Set1(prob_skill_and_US_normalized),\n",
    "                                  edgecolor = 'k',\n",
    "                                  width = .7,\n",
    "                                  ylim=[0,1])\n",
    "# Add super title\n",
    "fig.suptitle('Normalized Skill Frequency In Job Posts', fontsize=50, y=1.05)\n",
    "\n",
    "# Add the labels to the plots y-axes\n",
    "ax[0].set_ylabel('NY',fontsize=40)\n",
    "ax[1].set_ylabel('US',fontsize=40)\n",
    "\n",
    "# Add Gridlines\n",
    "ax[0].grid(b=None, which='major', axis='both')\n",
    "ax[1].grid(b=None, which='major', axis='both')\n",
    "\n",
    "# Make the subplot layout tightly fit\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('figures/NY_vs_US_normalized_hist.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now what is the same for these two normalized plots?\n",
    "- ## As I previously speculated, the distribution profiles are very similar.\n",
    "- ## Above all, Python, R and SQL are still dominating in demand in both the NY and the US dataset, and hadoop, java, and spark are are next in line.\n",
    "- ## However, there are some subtle differences in these plots, and we can show this by taking the difference between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the difference\n",
    "diff_NY_and_US = (prob_skill_and_NY_normalized-prob_skill_and_US_normalized).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the subplots\n",
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(25,15))\n",
    "\n",
    "\n",
    "# Make the colorbar\n",
    "## Since the color bar spans between positive and negative numbers,\n",
    "## we will get the largest absolute value from the maximum and minimum \n",
    "# and use it to make a symmetrical colorbar\n",
    "\n",
    "biggest_abs_value = max(\n",
    "    np.abs(min(diff_NY_and_US.values)), # get the abs value of smallest value from the set \n",
    "    np.abs(max(diff_NY_and_US.values))) # get the abs values of largest value from the set\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = -biggest_abs_value,\n",
    "                            vmax = biggest_abs_value)\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.Set1\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the node_weights and edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "bar_colors = m.to_rgba(diff_NY_and_US.values)\n",
    "\n",
    "\n",
    "\n",
    "# Add the plots to the subplots\n",
    "diff_NY_and_US.plot(ax =ax,kind='bar',\n",
    "                    fontsize=30,\n",
    "                    color=bar_colors,\n",
    "                    edgecolor='k',\n",
    "                    width = 0.9,\n",
    "                   ylim = [-biggest_abs_value,biggest_abs_value])\n",
    "\n",
    "# Add the labels to the plots y-axes\n",
    "ax.set_ylabel('NY - US',fontsize=40)\n",
    "\n",
    "# Add Gridlines\n",
    "ax.grid(b=None, which='major', axis='both')\n",
    "\n",
    "# Add super title\n",
    "fig.suptitle('Difference of Normalized Skill Frequency in Job Posts', fontsize=50)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('figures/NY_vs_US_diff_hist.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What differences do we see?\n",
    "- ## New York has a greater preference for big data in comparison to the whole country, as the leading positive differences are Spark, Scala, and Hadoop (all relating to big data). There are also positive differences for modern database technologies (Hive, pig, mongodb, cassandra, and the list goes on). \n",
    "- ## On the other hand, New York cares less about data visualization technologies like tableau, excel, and d3, than does the rest of the country, and it is consistent to say that they are less interested in a traditional database like oracle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #2: What skills appear together? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Lets look at the probability that a skill appears together with another skill ( In NY ;) ).\n",
    "\n",
    "- ## Say we wanted to see what skills we should learn together, this might be the kind of data that we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the combinations of skills that appear together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###  we use the *itertools.combinaitons( )* library to find all the combintions of pairs where r = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the individual skills\n",
    "skills_NY = list(data_NY.columns) \n",
    "\n",
    "# Get the combination of pairs of skills\n",
    "skill_pairs = [x for x in itertools.combinations(skills_NY, r = 2)]\n",
    "\n",
    "# There will be a total of around 1000 new columns that we will generate\n",
    "print(str(len(skill_pairs))+' combinations considered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###  Now we will make a new column for each combination of skills.\n",
    "- ### The value of a single row in the new column will be a 1 when the corresponding row from each column of the combination is 1.\n",
    "- ### In otherwords... the value of a row is 1 when a combination of skills all appear together in a job post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every pair, take the product of the columns considered\n",
    "# When row values are 1, the product will be 1 \n",
    "# Otherwise when row values are not all 1, the product will be 0\n",
    "\n",
    "# Initialize empty dataframe\n",
    "pairs_NY = pd.DataFrame()\n",
    "\n",
    "for each_pair in skill_pairs:\n",
    "\n",
    "    #Store the product between pairs columns\n",
    "    new_column = data_NY[list(each_pair)[0]]*data_NY[list(each_pair)[1]]\n",
    "    \n",
    "    # Add the new column to the dataframe\n",
    "    pairs_NY[str(each_pair)] = new_column \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets make the graph for the pairs of skills that occur together in  NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all NY cooccurances\n",
    "\n",
    "# Get the nodes\n",
    "nodes = skills_NY\n",
    "\n",
    "# Get the node weights\n",
    "node_weights = data_NY.mean().values\n",
    "\n",
    "# Get the pairs\n",
    "edges = [eval(x) for x in list(pairs_NY.keys())]\n",
    "\n",
    "# Get the pair weights\n",
    "edge_weights = pairs_NY.mean().values\n",
    "\n",
    "# Initialize a Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "plt.title('Data Scientist Skills in NY Job Posts',fontsize=50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)\n",
    "\n",
    "# Coloring the graph\n",
    "## We create a normal that will be used to color the edges that will need a max and min \n",
    "## value\n",
    "\n",
    "## We will scale the normal according the the maximum of the NY pair data and set the minimum to zero\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = 0,\n",
    "                            vmax = max(edge_weights))\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.Set2\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "## that we created in the colormap object m\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "edge_colors = m.to_rgba(edge_weights)\n",
    "\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        width = [200*x for x in edge_weights],\n",
    "        edge_color = edge_colors,\n",
    "        with_labels = True,\n",
    "        \n",
    "        # Include node weights for nodes in graph\n",
    "        node_size = [150000*x for x in node_weights],\n",
    "        node_color = 'peachpuff',\n",
    "        \n",
    "        # Make the edges and nodes transparent            \n",
    "        alpha=.95,       \n",
    "        font_size=50,\n",
    "        font_color='k')\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "m._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(m)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('Probability',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/NY_graph.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we read this graph?\n",
    "- ## The size of the nodes in the graph show how often skills appear on a job post\n",
    "- ## The width of the connecting lines show how often the skills appear together in a job post\n",
    "- ## The colors of the lines show lines with similar width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we see?\n",
    "- ## I see a lot of clutter and a lot of skills that don't appear together very often, so lets remove some less important skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function that prunes the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneNodes(threshold,pairs_data_mean_values,skills):\n",
    "    '''\n",
    "    The input of pruneNodes is 'threshold', a value that is an upper probability limit,\n",
    "    'pairs_data_mean_values' that is the dataframe of the mean for each of the paired skills,\n",
    "    and 'skills' that is a list of stings for all of the individual skills.\n",
    "    \n",
    "    This function prunes all of skills and skill pairs that have all mean pairs_data values \n",
    "    below the threshold\n",
    "    \n",
    "    The return values are the pruned list of skill pairs and the pruned list of skills\n",
    "\n",
    "    '''\n",
    "    # Initialize an empty dictionary for the skills and lists of skill pairs\n",
    "    skills_dictionary = {}\n",
    "\n",
    "    # Get the groups of all possible pairs for each skill\n",
    "    grouped_pairs = [[x for x in pairs_data_mean_values.keys() if y in eval(x)] for y in skills]\n",
    "    count=0\n",
    "\n",
    "    # Insert the list of skill pairs into a dictionary with the skills as keys\n",
    "    for skill in skills:\n",
    "        skills_dictionary[skill] = grouped_pairs[count]\n",
    "        count+=1\n",
    "\n",
    "    # If no edge weight inside of a node group is greater than the threshold value delete the \n",
    "    # column of that skill and all entries of that skill in other columns\n",
    "    count = 0\n",
    "    for skill in list(skills_dictionary.keys()):\n",
    "        count += 1\n",
    "        # if an entry in a group is higher than the threshold, \n",
    "        # then a false will appear in the group of skill pairs \n",
    "        # and that node is kept\n",
    "        if False not in [x for x in pairs_data_mean_values[skills_dictionary[skill]]<threshold if x==False]:\n",
    "            # Remove the node\n",
    "            del skills_dictionary[skill]\n",
    "            # Remove all of the entries of that node in the other dictionary entries by\n",
    "            # remaking a list for every remaining list in the dictionary\n",
    "            for x in list(skills_dictionary.keys()): \n",
    "                skills_dictionary[x] = [y for y in skills_dictionary[x] if skill not in eval(y)]\n",
    "\n",
    "    # Get all of the pairs of skills with no redundancy\n",
    "    pairs = []\n",
    "    [pairs.append(x) for y in list(skills_dictionary.keys()) for x in skills_dictionary[y] if x not in pairs]\n",
    "    \n",
    "    # Get all remaining skills\n",
    "    skills = list(skills_dictionary.keys())\n",
    "    \n",
    "    return pairs, skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will remove any skill node that has all cooccurance frequencies that is below the value of the 95th percentile of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the theshold quantile for the data \n",
    "threshold = pairs_NY.mean().quantile(.95)\n",
    "\n",
    "# Remove the insignifigant factors i.e, those that \n",
    "# don't have a probability higher than a threshold \n",
    "# value for all of their edges\n",
    "pruned_pairs, pruned_skills_NY = pruneNodes(threshold,\n",
    "                                            pairs_NY.mean(),\n",
    "                                            skills_NY)\n",
    "\n",
    "# Get the nodes\n",
    "nodes = pruned_skills_NY\n",
    "\n",
    "# Get the node weights\n",
    "node_weights = data_NY[pruned_skills_NY].mean().values\n",
    "\n",
    "# Get the pairs\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "edge_weights = pairs_NY[pruned_pairs].mean().values\n",
    "\n",
    "# Initialize a Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "plt.title('Data Scientist Skills in NY Job Posts (Pruned)',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)\n",
    "\n",
    "# Coloring the graph\n",
    "## We create a normal that will be used to color the edges that will need a max and min \n",
    "## value\n",
    "\n",
    "## We will scale the normal according the the maximum of the NY pair data and set the minimum to zero\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = 0,\n",
    "                            vmax = max(edge_weights))\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.Set2\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "## that we created in the colormap object m\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "edge_colors = m.to_rgba(edge_weights)\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        width = [300*x for x in edge_weights],\n",
    "        edge_color = edge_colors,\n",
    "        with_labels = True,\n",
    "        \n",
    "        # Include node weights for nodes in graph\n",
    "        node_size = [150000*x for x in node_weights],\n",
    "        node_color = 'peachpuff',\n",
    "        \n",
    "        # Make the edges and nodes transparent            \n",
    "        alpha=.95,       \n",
    "        font_size=70,\n",
    "        font_color='k')\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "m._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(m)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('Probability',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/NY_graph_pruned.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we see now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, lets look at what skills occur together with Python:\n",
    "- ## We see that Python and SQL occur together the most. \n",
    "- ## We see that Python and Spark, Python and Hadoop, and Python and Java, all occur the second most with the same frequency (This is an enlightening observation) :\n",
    "    - ### Even though Hadoop is written in Java, Python and Hadoop appear together more frequently than Java and Hadoop. This indicates that there is most likely support for using Hadoop with Python and that it is in demand ( Hadoop Streaming, Hadoopy, Pydoop, and MRJob -  https://www.thomashenson.com/hadoop-python-example/) \n",
    "    - ### Furthermore, even though Spark is written in Scala, Python and Spark appear together more frequently than Spark and Scala. This also indicates that there is most likely support for using Spark with Python and it is in demand (PySpark - https://www.udemy.com/spark-and-python-for-big-data-with-pyspark/).\n",
    "\n",
    "# Second, lets look at what skills occur together with R:\n",
    "- ## We see that R and SQL occur together the most. \n",
    "- ## We see that R and Spark and R and Hadoop appear the seconds most with the same frequency (This is also an enlightening observation):\n",
    "    - ### We can make the same argument. Even though Hadoop is written in Java, R and Hadoop appear together more frequently than Java and Hadoop. This indicates that there are most likely support for using Hadoop with R and it is in demand. (Hadoop Streaming, RHadoop, RHipe, ORCH, IBM BIGINSIGHTS BIG R).\n",
    "    - ### Again, even though Spark is written in Scala, R and Spark appear together more frequently than Spark and Scala. This also indicates that there are most likely support for using Spark with R (sparklyr - https://www.datacamp.com/courses/introduction-to-spark-in-r-using-sparklyr).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets compare the cooccurance of skills in NY with the cooccurance of skills with the rest of the united states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## To do this, we will normalize the occurance and cooccurance data together in the NY dataset and the US dataset to be between 0 and 1 with the same equation used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the combinations of skills that appear together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###  we use the *itertools.combinaitons( )* library to find all the combintions of pairs where r = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the individual skills\n",
    "skills_US = list(data_US.columns)\n",
    "\n",
    "# Get the combination of pairs of skills\n",
    "skill_pairs = [x for x in itertools.combinations(skills_US, r = 2)]\n",
    "\n",
    "# There will be a total of around 1000 new columns that we will generate\n",
    "print(str(len(skill_pairs))+' combinations considered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###  Now we will make a new column for each combination of skills.\n",
    "- ### The value of a single row in the new column will be a 1 when the corresponding row from each column of the combination is 1.\n",
    "- ### In otherwords... the value of a row is 1 when a combination of skills all appear together in a job post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every pair, take the product of the columns considered\n",
    "# When row values are 1, the product will be 1 \n",
    "# Otherwise when row values are not all 1, the product will be 0\n",
    "\n",
    "# Initialize empty dataframe\n",
    "pairs_US = pd.DataFrame()\n",
    "\n",
    "for each_pair in skill_pairs:\n",
    "\n",
    "    #Store the product between pairs columns\n",
    "    new_column = data_US[list(each_pair)[0]]*data_US[list(each_pair)[1]]\n",
    "    \n",
    "    # Add the new column to the dataframe\n",
    "    pairs_US[str(each_pair)] = new_column \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets get the graph for the pairs of skills that occur together for the Entire US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the theshold quantile for the data \n",
    "threshold = pairs_US.mean().quantile(0)\n",
    "\n",
    "# Remove the insignifigant factors i.e, those that \n",
    "# don't have a probability higher than a threshold \n",
    "# value for all of their edges\n",
    "pruned_pairs, pruned_skills_US = pruneNodes(threshold,\n",
    "                                            pairs_US.mean(),\n",
    "                                            skills_US)\n",
    "# Get the nodes\n",
    "nodes = pruned_skills_US\n",
    "\n",
    "# Get the node weights\n",
    "node_weights = data_US[pruned_skills_US].mean().values\n",
    "\n",
    "# Get the pairs\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "edge_weights = pairs_US[pruned_pairs].mean().values\n",
    "\n",
    "# Initialize a Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "plt.title('Data Scientist Skills in US Job Posts',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)\n",
    "                    \n",
    "\n",
    "# Coloring the graph\n",
    "## We create a normal that will be used to color the edges that will need a max and min \n",
    "## value\n",
    "\n",
    "## We will scale the normal according the the maximum of the NY pair data and set the minimum to zero\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = 0,\n",
    "                            vmax = max(edge_weights))\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.Set2\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "## that we created in the colormap object m\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "edge_colors = m.to_rgba(edge_weights)\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        width = [300*x for x in edge_weights],\n",
    "        edge_color = edge_colors,\n",
    "        with_labels = True,\n",
    "        \n",
    "        # Include node weights for nodes in graph\n",
    "        node_size = [150000*x for x in node_weights],\n",
    "        node_color = 'peachpuff',\n",
    "        \n",
    "        # Make the edges and nodes transparent            \n",
    "        alpha=.95,       \n",
    "        font_size=70,\n",
    "        font_color='k')\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "m._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(m)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('Probability',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/US_graph.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we see?\n",
    "- ## Just as in the NY graph, I see a lot of clutter and a lot of skills that don't appear together very often, so lets remove some less important skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the theshold quantile for the data \n",
    "threshold = pairs_US.mean().quantile(.95)\n",
    "\n",
    "# Remove the insignifigant factors i.e, those that \n",
    "# don't have a probability higher than a threshold \n",
    "# value for all of their edges\n",
    "pruned_pairs, pruned_skills_US = pruneNodes(threshold,\n",
    "                                            pairs_US.mean(),\n",
    "                                            skills_US)\n",
    "# Get the nodes\n",
    "nodes = pruned_skills_US\n",
    "\n",
    "# Get the node weights\n",
    "node_weights = data_US[pruned_skills_US].mean().values\n",
    "\n",
    "# Get the pairs\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "edge_weights = pairs_US[pruned_pairs].mean().values\n",
    "\n",
    "# Initialize a Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "plt.title('Requested Data Scientist Skills in US (Pruned)',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)\n",
    "                    \n",
    "\n",
    "# Coloring the graph\n",
    "## We create a normal that will be used to color the edges that will need a max and min \n",
    "## value\n",
    "\n",
    "## We will scale the normal according the the maximum of the NY pair data and set the minimum to zero\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = 0,\n",
    "                            vmax = max(edge_weights))\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.Set2\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "## that we created in the colormap object m\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "edge_colors = m.to_rgba(edge_weights) # Use the same scale as NY\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        width = [600*x for x in edge_weights],\n",
    "        edge_color = edge_colors,\n",
    "        with_labels = True,\n",
    "        \n",
    "        # Include node weights for nodes in graph\n",
    "        node_size = [150000*x for x in node_weights],\n",
    "        node_color = 'peachpuff',\n",
    "        \n",
    "        # Make the edges and nodes transparent            \n",
    "        alpha=.95,       \n",
    "        font_size=70,\n",
    "        font_color='k')\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "m._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(m)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('Probability',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/US_graph_pruned.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we see now?\n",
    "\n",
    "# First, lets look at what occurs together with Python:\n",
    "- ## We see that Python and SQL occur together the most. \n",
    "- ## We see that Python and Spark, Python and Hadoop, and Python and Java, all occur with the same frequency (This is an enlightening observation) :\n",
    "    - ### Even though Hadoop is written in Java, Python and Hadoop appear together more frequently than Java and Hadoop. This indicates that there are most likely supporting libraries for Hadoop in Python. \n",
    "    - ### Second, even though Spark is written in Scala, Python and Spark appear together more frequently than Spark and Scala. This also indicates that there are most likely supporting libraries for Spark in Python. \n",
    "\n",
    "# Second, lets look at what occurs with R:\n",
    "- ## We see that R and SQL occur together the most. \n",
    "- ## We see that R and SAS occur together the second most.\n",
    "- ## We see that R and Spark, R and Hadoop, R and Java, and R and Tableau, and a few other things appear together with the same frequency (This is also an enlightening observation):\n",
    "    - ### Again, even though Hadoop is written in Java, R and Hadoop appear together more frequently than Java and Hadoop. This indicates that there are most likely supporting libraries for Hadoop in R. \n",
    "    - ### Again, even though Spark is written in Scala, R and Spark appear together more frequently than Spark and Scala. This also indicates that there are most likely supporting libraries for Spark in R. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VVVVVV FIX Colormap VVVVVV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do the pairs of skills that occur together for NY compare to the pairs of skills that occur together in the entire US?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First get the Graph for the Normalized Cooccurance of skills in NY and Normalized occurance of skills in NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized cooccurance data for skills in NY\n",
    "cooc_data_NY_normalized = normalizeZeroOne(pairs_NY.mean())\n",
    "\n",
    "# Get the normalized occurance data for skills in NY\n",
    "data_NY_normalized = normalizeZeroOne(data_NY.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the normalized NY data\n",
    "\n",
    "# Prune the data for all cooccurance values that are less than the threshold in pruneNodes\n",
    "pruned_pairs, pruned_skills = pruneNodes(0, # The threshold\n",
    "                                            cooc_data_NY_normalized, # The normalized pair data for NY\n",
    "                                            skills_NY) # The skills list for NY\n",
    "\n",
    "# Get Nodes\n",
    "nodes = pruned_skills\n",
    "\n",
    "# Get tuples of names for the pairs of skills\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "weights = cooc_data_NY_normalized.values\n",
    "\n",
    "# Make the graph figure\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "plt.title('Requested Data Scientist Skills and Their Pairs in NY Normalized',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)\n",
    "                \n",
    "# Draw the graph\n",
    "nx.draw(G,\n",
    "        graph_pos,         \n",
    "        width = [200*x for x in weights],\n",
    "        edge_cmap = plt.cm.Set2,\n",
    "        \n",
    "        # edge_color values will assign the color interval from weights\n",
    "        # that spans from min(weights) to max(weights)\n",
    "        edge_color = plt.cm.Set2(weights),\n",
    "        with_labels = True,\n",
    "        node_size = [200000*x for x in data_NY_normalized[nodes]],\n",
    "        \n",
    "        # cmap for nodes\n",
    "        cmap = plt.get_cmap('Set2'),\n",
    "        node_color = plt.cm.Set2(data_NY_normalized[nodes].values),\n",
    "\n",
    "        # Make the edges and nodes transparent            \n",
    "        alpha=.95,\n",
    "        font_size=50,\n",
    "        font_color='k')\n",
    "\n",
    "# Make a colorbar for the graph\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.Set2,\n",
    "                           # Set the values placed in the colorbar\n",
    "                           norm=plt.Normalize(vmin=0,\n",
    "                                              vmax=1))\n",
    "                          \n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "sm._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(sm)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('Normalized Probability',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/NY_normalized_graph.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second get the Graph for the Normalized Cooccurance of skills in the US and Normalized occurance of skills in the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized cooccurance data for skills in NY\n",
    "cooc_data_US_normalized = normalizeZeroOne(pairs_US.mean())\n",
    "\n",
    "# Get the normalized occurance data for skills in NY\n",
    "data_US_normalized = normalizeZeroOne(data_US.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized NY data\n",
    "\n",
    "# Prune the data for all cooccurance values that are less than the threshold in pruneNodes\n",
    "pruned_pairs, pruned_skills = pruneNodes(0, # The threshold\n",
    "                                            cooc_data_US_normalized, # The normalized pair data for NY\n",
    "                                            skills_US) # The skills list for NY\n",
    "# Get Nodes\n",
    "nodes = pruned_skills\n",
    "\n",
    "# Get tuples of names for the pairs of skills\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "weights = cooc_data_US_normalized.values\n",
    "\n",
    "# Make the graph figure\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "plt.title('Requested Data Scientist Skills and Their Pairs in US Normalized',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)\n",
    "                \n",
    "# Draw the graph\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        width = [200*x for x in weights],\n",
    "        edge_cmap = plt.cm.Set2,\n",
    "        \n",
    "        # edge_color values will assign the color interval from weights\n",
    "        # that spans from min(weights) to max(weights)\n",
    "        edge_color = plt.cm.Set2(weights),\n",
    "        with_labels = True,\n",
    "        node_size = [200000*x for x in data_US_normalized[nodes]],\n",
    "        \n",
    "        # cmap for nodes\n",
    "        cmap = plt.get_cmap('Set2'),\n",
    "        node_color = plt.cm.Set2(data_US_normalized[nodes].values),\n",
    "\n",
    "        # Make the edges and nodes transparent            \n",
    "        alpha=.95,\n",
    "        font_size=50,\n",
    "        font_color='k')\n",
    "\n",
    "# Make a colorbar for the graph\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.Set2,\n",
    "                           # Set the values placed in the colorbar\n",
    "                           norm=plt.Normalize(vmin=0,\n",
    "                                              vmax=1))\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "sm._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(sm)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('Normalized Probability',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/US_normalized_graph.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ^^^^^^  FIX Colormap ^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third get the graph for the Normalized Cooccurance of Skills in NY minus the Normalized Cooccurance of Skills in the US and the Normalized Occurance of Skills in NY minus the Normalized Occurance of Skills in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the difference in the normalized Cooccurance data between NY and the US\n",
    "cooc_data_NY_US_normalized_diff = cooc_data_NY_normalized - cooc_data_US_normalized\n",
    "\n",
    "# Get the difference in the normalized Occurance data between NY and the US\n",
    "data_NY_US_normalized_diff = data_NY_normalized - data_US_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the normalized NY minus US data\n",
    "\n",
    "# Make the threshold the xth percentile of all the values \n",
    "# when they are made to be positive\n",
    "threshold = cooc_data_NY_US_normalized_diff.abs().quantile(0)\n",
    "\n",
    "# Get the pruned data\n",
    "pruned_pairs, pruned_skills_NY_US = pruneNodes(threshold, # The threshold\n",
    "                                            cooc_data_NY_US_normalized_diff.abs(), # The absolute value of normalized pair data for NY minus US\n",
    "                                            list(data_NY_US_normalized_diff.keys())) # The skills list for NY and US\n",
    "# Get Nodes\n",
    "nodes = pruned_skills_NY_US\n",
    "\n",
    "# Get Node weights\n",
    "\n",
    "node_weights = data_NY_US_normalized_diff[pruned_skills_NY_US].values\n",
    "\n",
    "# Get the pairs\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "edge_weights = cooc_data_NY_US_normalized_diff[pruned_pairs].values\n",
    "\n",
    "# Make the graph figure\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "# plt.title('Difference in Normalized Skill Frequency and Cooccurance of skills for US and NY',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)                 \n",
    "\n",
    "# Coloring the graph\n",
    "\n",
    "## We create a normal that will be used to color the nodes that will need a max and min \n",
    "## value\n",
    "\n",
    "## Since the nodes and edges will be colored according to the same color bar, we will\n",
    "## take the max and min from both sets combined and\n",
    "## we will get the largest absolute value and use the positive and negative of that\n",
    "## to make the colorbar symetrical\n",
    "\n",
    "biggest_abs_value = max(\n",
    "    np.abs(min(min(edge_weights),min(node_weights))), # get the smallest value from both sets\n",
    "    np.abs(max(max(edge_weights),max(node_weights)))) # get the largest value from both sets\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = -biggest_abs_value,\n",
    "                            vmax = biggest_abs_value)\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.RdBu_r\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the node_weights and edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "node_colors = m.to_rgba(node_weights)\n",
    "edge_colors = m.to_rgba(edge_weights)\n",
    "\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        \n",
    "        # The values of the weights are both negative and positive, so the width will \n",
    "        # adjust according to the normalized absolute value of the width values\n",
    "        width = [100*x for x in normalizeZeroOne(np.abs(edge_weights))],\n",
    " \n",
    "        edge_color = edge_colors,\n",
    "        \n",
    "        # Add labels to the nodes\n",
    "        with_labels = True,\n",
    "        \n",
    "        # Take the absolute value of the differences\n",
    "        node_size = [abs(800000*x) for x in node_weights],\n",
    "        \n",
    "        # Use red to indicate positive and blue to indicate negative\n",
    "        node_color = node_colors,\n",
    "        \n",
    "        # Make the edges and nodes transparent\n",
    "        alpha=.95,\n",
    "        font_size=50,\n",
    "        font_color='k')\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar\n",
    "m._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(m)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('(Percent Preference of NY vs. US)/100',fontsize=50)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 40.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/US_NY_normalized_diff_graph.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What differences do we see? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again, this looks like a tangled mess. So lets prune most of the less interesting stuff, like the skills with cooccurance values strictly between -.045 and .045."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth prune some of the smaller differences from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the normalized and pruned NY minus US data\n",
    "\n",
    "# Make the threshold the xth percentile of all the values \n",
    "# when they are made to be positive\n",
    "threshold = cooc_data_NY_US_normalized_diff.abs().quantile(.98)\n",
    "\n",
    "pruned_pairs, pruned_skills_NY_US = pruneNodes(threshold, # The threshold\n",
    "                                            cooc_data_NY_US_normalized_diff.abs(), # The absolute value of normalized pair data for NY minus US\n",
    "                                            list(data_NY_US_normalized_diff.keys())) # The skills list for NY and US\n",
    "# Get Nodes\n",
    "nodes = pruned_skills_NY_US\n",
    "\n",
    "# Get Node weights\n",
    "\n",
    "node_weights = data_NY_US_normalized_diff[pruned_skills_NY_US].values\n",
    "\n",
    "# Get the pairs\n",
    "edges = [eval(x) for x in pruned_pairs]\n",
    "\n",
    "# Get the pair weights\n",
    "edge_weights = cooc_data_NY_US_normalized_diff[pruned_pairs].values\n",
    "\n",
    "# Make the graph figure\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the list of names of the nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Add the list of names of the edges\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjust the figure size\n",
    "plt.figure(figsize=(50,40))\n",
    "\n",
    "# Add Title\n",
    "# plt.title('Difference in Normalized Skill Frequency and Cooccurance of Skills for NY and US (Pruned)',fontsize = 50)\n",
    "\n",
    "# Use Fruchterman-Reingold force-directed algorithm to position the nodes.\n",
    "graph_pos = nx.spring_layout(G)                 \n",
    "\n",
    "# Coloring the graph\n",
    "## We create a normal that will be used to color the nodes that will need a max and min \n",
    "## value\n",
    "\n",
    "## Since the nodes and edges will be colored according to the same color bar, we will\n",
    "## take the max and min from both sets combined and\n",
    "## we will get the largest absolute value and use the positive and negative of that\n",
    "## to make the colorbar symetrical\n",
    "\n",
    "biggest_abs_value = max(\n",
    "    np.abs(min(min(edge_weights),min(node_weights))), # get the smallest value from both sets\n",
    "    np.abs(max(max(edge_weights),max(node_weights)))) # get the largest value from both sets\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin = -biggest_abs_value,\n",
    "                            vmax = biggest_abs_value)\n",
    "\n",
    "## We select the colormap that we want\n",
    "cmap = mpl.cm.RdBu_r\n",
    "\n",
    "## Now we get the RGBA values using our norm and cmap for the node_weights and edge_weights\n",
    "## where the values will color according to where their values fall in the normalized scale\n",
    "m = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "node_colors = m.to_rgba(node_weights)\n",
    "edge_colors = m.to_rgba(edge_weights)\n",
    "\n",
    "nx.draw(G,\n",
    "        graph_pos, \n",
    "        # The values of the weights are both negative and positive, so the width will \n",
    "        # adjust according to the normalized absolute value of the width values\n",
    "        width = [100*x for x in normalizeZeroOne(np.abs(edge_weights))],\n",
    " \n",
    "        edge_color = edge_colors,\n",
    "        \n",
    "        # Add labels to the nodes\n",
    "        with_labels = True,\n",
    "        \n",
    "        # Take the absolute value of the differences\n",
    "        node_size = [abs(800000*x) for x in node_weights],\n",
    "        \n",
    "        # Use red to indicate positive and blue to indicate negative\n",
    "        node_color = node_colors,\n",
    "        \n",
    "        # Make the edges and nodes transparent\n",
    "        alpha=.95,\n",
    "        font_size=70,\n",
    "        font_color='k')\n",
    "\n",
    "# Not sure what this does, but it is necessary to plot the colorbar to plot the colorbar\n",
    "m._A = []\n",
    "\n",
    "# Add the colorbar \n",
    "ax = plt.colorbar(m)\n",
    "\n",
    "# Add label to colorbar\n",
    "ax.set_label('(Percent Preference of NY vs US)/100',fontsize=70)\n",
    "\n",
    "# Change the font size of the ticks\n",
    "mpl.rcParams['font.size'] = 50.0\n",
    "mpl.rcParams.update()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/NY_US_normalized_diff_pruned_graph.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
