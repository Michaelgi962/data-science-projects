{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref: https://codelike.pro/create-a-crawler-with-rotating-ip-proxy-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text data from the first job posting\n",
    "import pandas as pd\n",
    "import requests # For website connections\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Https_txt_file = 'proxyLists/https_proxys_part_0.txt'\n",
    "    #Download New HTTPS IPs .txt file from https://www.proxy-list.download/HTTPS\n",
    "    https_proxys = pd.read_csv(Https_txt_file, sep=\" \", header=None)\n",
    "    #Convert to array\n",
    "    https_proxys = https_proxys[0].values\n",
    "    #Convert to list\n",
    "    https_proxys = list(https_proxys)\n",
    "except:\n",
    "    print('There are no https proxies to import')\n",
    "\n",
    "try:\n",
    "    Http_txt_file = 'proxyLists/http_proxys_part_0.txt'\n",
    "    #Download New HTTPS IPs .txt file from https://www.proxy-list.download/HTTP\n",
    "    http_proxys = pd.read_csv(Http_txt_file, sep=\" \", header=None)\n",
    "    #Convert to array\n",
    "    http_proxys = http_proxys[0].values\n",
    "    #Convert to list\n",
    "    http_proxys = list(http_proxys)\n",
    "except:\n",
    "    print('There are no http proxies to import')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for the proxies to test\n",
    "testProxies = {'https':https_proxys,'http':http_proxys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a random user-agent\n",
    "header = {}\n",
    "def randHeader():\n",
    "    ua = UserAgent()\n",
    "    header['User-Agent'] = ua.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a random index for the https proxy\n",
    "def randomIndex(conn_type):\n",
    "    num = random.randint(0, len(testProxies[conn_type]) - 1)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxyDict = {}\n",
    "def randProx(conn_type):\n",
    "    # Choose a single random proxy to use\n",
    "    proxy_index = randomIndex(conn_type)\n",
    "    proxy = testProxies[conn_type][proxy_index]\n",
    "    proxyDict[conn_type] = conn_type +'://'+ proxy\n",
    "    return proxy_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dictionary for good proxies \n",
    "good_http_proxy = []\n",
    "good_https_proxy = []\n",
    "conn_time_http = []\n",
    "conn_time_https = []\n",
    "goodProxy = {'https':{'proxy':good_https_proxy,'time':conn_time_https},\n",
    "             'http':{'proxy':good_http_proxy,'time':conn_time_http}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterProxys(url):\n",
    "    timer = 0\n",
    "    count = 0\n",
    "    if url[0:5] == 'http:':\n",
    "        conn_type = 'http'\n",
    "        \n",
    "    elif url[0:5] == 'https':\n",
    "        conn_type = 'https'\n",
    "\n",
    "    total_proxies = len(testProxies[conn_type])\n",
    "    for n in range(len(testProxies[conn_type])):\n",
    "        time.sleep(1)\n",
    "        print(str(total_proxies-n)+' proxies left to filter')\n",
    "            \n",
    "        try:\n",
    "            # Choose a random header\n",
    "            randHeader()\n",
    "            \n",
    "        except:\n",
    "            # Sometimes the header reassignment function returns an error\n",
    "            pass\n",
    "            \n",
    "        # Choose a random proxy\n",
    "        proxy_index = randProx(conn_type)\n",
    "\n",
    "        try:\n",
    "            # Try to connect to the url\n",
    "            t1 = time.time()\n",
    "            soup = requests.get(url,headers=header,proxies=proxyDict,timeout=5)\n",
    "            t2 = time.time()\n",
    "            t = t2-t1\n",
    "                \n",
    "            if  str(soup) == '<Response [200]>': # the connection is successful when <Response [200]> is returned\n",
    "                print(str(testProxies[conn_type][proxy_index])+' is good')\n",
    "                del testProxies[conn_type][proxy_index]\n",
    "                timer += t\n",
    "                count += 1\n",
    "                print('average conn time: '+str(timer/count)+'\\n')\n",
    "                #Add to the good proxy and its connection time\n",
    "                goodProxy[conn_type]['proxy'].append(proxyDict[conn_type])\n",
    "                goodProxy[conn_type]['time'].append(t)\n",
    "                    \n",
    "            elif str(soup) != '<Response [200]>': #The connection was bad\n",
    "                print(str(testProxies[conn_type][proxy_index])+' is bad'+'\\n')\n",
    "                del testProxies[conn_type][proxy_index]\n",
    "                    \n",
    "                    \n",
    "        except: # The connection took too long \n",
    "            print(str(testProxies[conn_type][proxy_index])+' is slow'+'\\n')\n",
    "            del testProxies[conn_type][proxy_index]\n",
    "    try:\n",
    "        print('average proxy time:'+str(timer/count))\n",
    "    except:\n",
    "        print('No proxies to text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Https proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 proxies left to filter\n",
      "162.155.10.150:53281 is bad\n",
      "\n",
      "225 proxies left to filter\n",
      "167.86.79.59:3128 is slow\n",
      "\n",
      "224 proxies left to filter\n",
      "159.89.190.135:8080 is slow\n",
      "\n",
      "223 proxies left to filter\n",
      "157.230.228.82:8080 is slow\n",
      "\n",
      "222 proxies left to filter\n",
      "206.221.177.135:3128 is slow\n",
      "\n",
      "221 proxies left to filter\n",
      "162.211.126.220:443 is slow\n",
      "\n",
      "220 proxies left to filter\n",
      "157.230.222.118:8080 is slow\n",
      "\n",
      "219 proxies left to filter\n",
      "159.89.141.36:8080 is slow\n",
      "\n",
      "218 proxies left to filter\n",
      "134.209.209.76:8080 is slow\n",
      "\n",
      "217 proxies left to filter\n",
      "138.197.102.119:80 is slow\n",
      "\n",
      "216 proxies left to filter\n",
      "134.209.123.111:21 is slow\n",
      "\n",
      "215 proxies left to filter\n",
      "157.230.227.116:80 is slow\n",
      "\n",
      "214 proxies left to filter\n",
      "157.230.233.67:3128 is good\n",
      "average conn time: 0.406543970108\n",
      "\n",
      "213 proxies left to filter\n",
      "142.93.251.20:8080 is slow\n",
      "\n",
      "212 proxies left to filter\n",
      "157.230.233.67:80 is good\n",
      "average conn time: 0.394755482674\n",
      "\n",
      "211 proxies left to filter\n",
      "157.230.140.12:80 is slow\n",
      "\n",
      "210 proxies left to filter\n",
      "165.227.107.218:80 is slow\n",
      "\n",
      "209 proxies left to filter\n",
      "134.209.109.196:8080 is slow\n",
      "\n",
      "208 proxies left to filter\n",
      "167.99.148.235:8080 is bad\n",
      "\n",
      "207 proxies left to filter\n",
      "157.230.228.82:3128 is slow\n",
      "\n",
      "206 proxies left to filter\n",
      "167.99.148.235:3128 is bad\n",
      "\n",
      "205 proxies left to filter\n",
      "192.119.203.170:48678 is slow\n",
      "\n",
      "204 proxies left to filter\n",
      "157.230.227.116:8080 is slow\n",
      "\n",
      "203 proxies left to filter\n",
      "165.22.142.58:3128 is good\n",
      "average conn time: 0.453018347422\n",
      "\n",
      "202 proxies left to filter\n",
      "178.128.186.2:8080 is slow\n",
      "\n",
      "201 proxies left to filter\n",
      "157.230.220.233:8080 is slow\n",
      "\n",
      "200 proxies left to filter\n",
      "157.230.222.118:80 is slow\n",
      "\n",
      "199 proxies left to filter\n",
      "104.248.119.90:8080 is slow\n",
      "\n",
      "198 proxies left to filter\n",
      "192.119.203.124:48678 is good\n",
      "average conn time: 1.50898551941\n",
      "\n",
      "197 proxies left to filter\n",
      "157.230.159.32:80 is slow\n",
      "\n",
      "196 proxies left to filter\n",
      "148.66.38.193:46506 is slow\n",
      "\n",
      "195 proxies left to filter\n",
      "134.209.108.155:31330 is bad\n",
      "\n",
      "194 proxies left to filter\n",
      "206.81.11.75:8080 is slow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.proxy-list.download/HTTPS'\n",
    "filterProxys(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Http proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filterProxys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5944368344cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.proxy-list.download/HTTP'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilterProxys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'filterProxys' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'https://www.proxy-list.download/HTTP'\n",
    "filterProxys(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite the source text file with partially filtered or empty ip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the existing source of https proxy data with the filtered data\n",
    "with open('proxyLists/https_proxys_part_0.txt', \"w\") as output:\n",
    "    for line in testProxies['https']:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the existing source of http proxy data with the filtered data\n",
    "with open('proxyLists/http_proxys_part_0.txt', \"w\") as output:\n",
    "    for line in testProxies['http']:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all proxys that had connections times less than one second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(goodProxy['https'])\n",
    "fast_proxies_https = list(df['proxy'][df['time']<1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(goodProxy['http'])\n",
    "fast_proxies_http = list(df['proxy'][df['time']<1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the single filtered files of proxys that had connections times less than one second and combine with the new filtered proxys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "try:\n",
    "    existing_https_proxies = list(pd.read_csv('../filtered_https_proxys.txt', header = None)[0])\n",
    "except: #If the file is empty\n",
    "    existing_https_proxies = []\n",
    "try:\n",
    "    existing_http_proxies = list(pd.read_csv('../filtered_http_proxys.txt', header = None)[0])\n",
    "except: #If the file is empty\n",
    "    existing_http_proxies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the new proxys to the existing ones if they are not already there\n",
    "[existing_https_proxies.append(x) \n",
    " for x in fast_proxies_https \n",
    " if x not in existing_https_proxies]\n",
    "\n",
    "[existing_http_proxies.append(x) \n",
    " for x in fast_proxies_http \n",
    " if x not in existing_http_proxies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite the final proxys text file with the new and old filtered proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append filtered http proxys to existing .txt file\n",
    "with open('../filtered_http_proxys.txt', \"w\") as output:\n",
    "    for line in existing_http_proxies:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append filtered https proxies to existing text file\n",
    "with open('../filtered_https_proxys.txt', \"w\") as output:\n",
    "    for line in existing_https_proxies:\n",
    "        output.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
